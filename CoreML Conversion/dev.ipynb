{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeOldify Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DeOldfy libraries\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from deoldify import device\n",
    "from deoldify.device_id import DeviceId\n",
    "from deoldify.visualize import *\n",
    "import warnings\n",
    "\n",
    "from matplotlib.pyplot import imshow\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Set Deoldfy configs\n",
    "device.set(device=DeviceId.CPU)\n",
    "torch.backends.cudnn.benchmark=True\n",
    "plt.style.use('dark_background')\n",
    "plt.style\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*?Your .*? set is empty.*?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DeOldfy Model\n",
    "colorizer = get_image_colorizer(artistic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Image.open('../test_images/image.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as tf\n",
    "\n",
    "transform_in = tf.Compose([\n",
    "    tf.Resize((560,560)),\n",
    "    tf.ToTensor(),\n",
    "    tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "transformed_img = transform_in(input_img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model\n",
    "with torch.no_grad():\n",
    "    output = colorizer.filter.filters[0].learn.model(transformed_img)[0]\n",
    "print(output.shape, output.data[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_out = tf.Compose([\n",
    "    tf.Normalize([0., 0., 0.], [1/0.229, 1/0.224, 1/0.225]),\n",
    "    tf.Normalize([-0.485, -0.456, -0.406], [ 1., 1., 1. ]),\n",
    "    tf.Resize((input_img.size[1],input_img.size[0])),\n",
    "    # lambda x: x*255,\n",
    "    tf.ToPILImage(),\n",
    "])\n",
    "transformed_output = transform_out(output)\n",
    "# output_img = transformed_output.permute(1,2,0).numpy().astype(np.uint8)\n",
    "# output_img = Image.fromarray(output_img)\n",
    "# output_img = output_img.resize(input_img.size, resample=Image.BILINEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "color_np = np.array(transformed_output)\n",
    "orig_np = np.array(input_img)\n",
    "color_yuv = cv2.cvtColor(color_np, cv2.COLOR_RGB2YUV)\n",
    "# do a black and white transform first to get better luminance values\n",
    "orig_yuv = cv2.cvtColor(orig_np, cv2.COLOR_RGB2YUV)\n",
    "hires = np.copy(orig_yuv)\n",
    "hires[:, :, 1:3] = color_yuv[:, :, 1:3]\n",
    "final = cv2.cvtColor(hires, cv2.COLOR_YUV2RGB)\n",
    "final = Image.fromarray(final)\n",
    "imshow(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TorchScript\n",
    "trace = torch.jit.trace(colorizer.filter.filters[0].learn.model, transformed_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to CoreML\n",
    "import coremltools as ct\n",
    "mlmodel = ct.convert(\n",
    "\ttrace,\n",
    "\tinputs=[ct.TensorType(name=\"input\", shape=transformed_img.shape)],\n",
    "    convert_to='neuralnetwork'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction using Core ML\n",
    "out_mlmodel = mlmodel.predict({\"input\": transformed_img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = out_mlmodel['var_1244'][0]\n",
    "\n",
    "output_img = transform_out(torch.tensor(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_np = np.array(output_img)\n",
    "orig_np = np.array(input_img)\n",
    "color_yuv = cv2.cvtColor(color_np, cv2.COLOR_RGB2YUV)\n",
    "# do a black and white transform first to get better luminance values\n",
    "orig_yuv = cv2.cvtColor(orig_np, cv2.COLOR_RGB2YUV)\n",
    "hires = np.copy(orig_yuv)\n",
    "hires[:, :, 1:3] = color_yuv[:, :, 1:3]\n",
    "final = cv2.cvtColor(hires, cv2.COLOR_YUV2RGB)\n",
    "final = Image.fromarray(final)\n",
    "f, axarr = plt.subplots(1,2, figsize=(40,40))\n",
    "axarr[0].imshow(input_img)\n",
    "axarr[0].axis('off')\n",
    "axarr[1].imshow(final)\n",
    "axarr[1].axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ColorNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
